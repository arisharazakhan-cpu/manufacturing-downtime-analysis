{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bafb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    precision_recall_curve,\n",
    "    average_precision_score,\n",
    "    roc_auc_score\n",
    ")\n",
    "\n",
    "# Load and clean\n",
    "df = pd.read_csv(\"../data/raw/ai4i2020.csv\")\n",
    "\n",
    "df.columns = (\n",
    "    df.columns\n",
    "    .str.lower()\n",
    "    .str.replace(\" \", \"_\")\n",
    "    .str.replace(\"[\", \"\", regex=False)\n",
    "    .str.replace(\"]\", \"\", regex=False)\n",
    ")\n",
    "\n",
    "# Feature engineering\n",
    "df[\"temp_diff\"] = df[\"process_temperature_k\"] - df[\"air_temperature_k\"]\n",
    "\n",
    "# Define target and remove leakage columns\n",
    "target = \"machine_failure\"\n",
    "failure_type_cols = [\"twf\", \"hdf\", \"pwf\", \"osf\", \"rnf\"]\n",
    "id_cols = [\"udi\", \"product_id\"]\n",
    "\n",
    "X = df.drop(columns=[target] + failure_type_cols + id_cols)\n",
    "y = df[target]\n",
    "\n",
    "categorical_features = [\"type\"]\n",
    "numeric_features = [c for c in X.columns if c not in categorical_features]\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.25,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "# Preprocess\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OneHotEncoder(drop=\"first\", handle_unknown=\"ignore\"), categorical_features),\n",
    "        (\"num\", \"passthrough\", numeric_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Models\n",
    "logreg = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocess\", preprocessor),\n",
    "        (\"model\", LogisticRegression(max_iter=2000, class_weight=\"balanced\"))\n",
    "    ]\n",
    ")\n",
    "\n",
    "rf = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocess\", preprocessor),\n",
    "        (\"model\", RandomForestClassifier(\n",
    "            n_estimators=400,\n",
    "            random_state=42,\n",
    "            class_weight=\"balanced_subsample\",\n",
    "            n_jobs=-1\n",
    "        ))\n",
    "    ]\n",
    ")\n",
    "\n",
    "def evaluate_model(name, pipeline):\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "    pred = (proba >= 0.5).astype(int)\n",
    "\n",
    "    ap = average_precision_score(y_test, proba)\n",
    "    roc = roc_auc_score(y_test, proba)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(name)\n",
    "    print(\"average_precision:\", round(ap, 4))\n",
    "    print(\"roc_auc:\", round(roc, 4))\n",
    "    print(classification_report(y_test, pred))\n",
    "    print(confusion_matrix(y_test, pred))\n",
    "\n",
    "    return proba\n",
    "\n",
    "proba_logreg = evaluate_model(\"Logistic Regression baseline\", logreg)\n",
    "proba_rf = evaluate_model(\"Random Forest\", rf)\n",
    "\n",
    "# Precision recall curves\n",
    "p_lr, r_lr, t_lr = precision_recall_curve(y_test, proba_logreg)\n",
    "p_rf, r_rf, t_rf = precision_recall_curve(y_test, proba_rf)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(r_lr, p_lr, label=\"logistic regression\")\n",
    "plt.plot(r_rf, p_rf, label=\"random forest\")\n",
    "plt.xlabel(\"recall\")\n",
    "plt.ylabel(\"precision\")\n",
    "plt.title(\"Precision Recall Curve\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7c04b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "def report_at_recall(name, proba, recall_target=0.85):\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, proba)\n",
    "\n",
    "    # thresholds length is one less than precision and recall\n",
    "    best_idx = None\n",
    "    for i in range(len(thresholds)):\n",
    "        if recall[i] >= recall_target:\n",
    "            best_idx = i\n",
    "            break\n",
    "\n",
    "    if best_idx is None:\n",
    "        print(\"\\n\" + name)\n",
    "        print(\"No threshold reaches recall target:\", recall_target)\n",
    "        return\n",
    "\n",
    "    thr = thresholds[best_idx]\n",
    "    preds = (proba >= thr).astype(int)\n",
    "\n",
    "    p = precision_score(y_test, preds, zero_division=0)\n",
    "    r = recall_score(y_test, preds)\n",
    "    cm = confusion_matrix(y_test, preds)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(name)\n",
    "    print(\"threshold:\", round(float(thr), 4))\n",
    "    print(\"precision:\", round(float(p), 4))\n",
    "    print(\"recall:\", round(float(r), 4))\n",
    "    print(cm)\n",
    "\n",
    "report_at_recall(\"Logistic Regression tuned\", proba_logreg, recall_target=0.85)\n",
    "report_at_recall(\"Random Forest tuned\", proba_rf, recall_target=0.85)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b82cbc3",
   "metadata": {},
   "source": [
    "Logistic regression provides a strong baseline and achieves high recall for failures when class weights are used, but it produces many false positives. A random forest improves ranking performance for rare failures, shown by higher average precision and a stronger precision recall curve. Instead of using the default 0.5 cutoff, the decision threshold can be tuned to prioritize recall, which is often preferable in preventive maintenance where missed failures are more costly than additional inspections. Naively selecting the first threshold that reaches a recall target can result in extreme operating points, so threshold selection should reflect realistic operational tradeoffs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98828939",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_threshold_for_min_recall(proba, recall_min=0.85):\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, proba)\n",
    "\n",
    "    # precision and recall have length thresholds+1\n",
    "    best = None\n",
    "    for i, thr in enumerate(np.append(thresholds, 1.0)):\n",
    "        p = precision[i]\n",
    "        r = recall[i]\n",
    "        if r >= recall_min:\n",
    "            if best is None or p > best[\"precision\"]:\n",
    "                best = {\"threshold\": float(thr), \"precision\": float(p), \"recall\": float(r)}\n",
    "    return best\n",
    "\n",
    "def report_threshold(name, proba, thr):\n",
    "    preds = (proba >= thr).astype(int)\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(name)\n",
    "    print(\"threshold:\", round(float(thr), 4))\n",
    "    print(classification_report(y_test, preds))\n",
    "    print(confusion_matrix(y_test, preds))\n",
    "\n",
    "best_lr = best_threshold_for_min_recall(proba_logreg, recall_min=0.85)\n",
    "best_rf = best_threshold_for_min_recall(proba_rf, recall_min=0.85)\n",
    "\n",
    "print(\"Best LR (min recall 0.85):\", best_lr)\n",
    "print(\"Best RF (min recall 0.85):\", best_rf)\n",
    "\n",
    "report_threshold(\"Logistic Regression tuned (min recall 0.85)\", proba_logreg, best_lr[\"threshold\"])\n",
    "report_threshold(\"Random Forest tuned (min recall 0.85)\", proba_rf, best_rf[\"threshold\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3563d78c",
   "metadata": {},
   "source": [
    "At a fixed recall target of approximately 0.86, the random forest significantly outperforms logistic regression by achieving substantially higher precision. This results in far fewer false positives while maintaining strong failure detection, making the random forest a more practical choice for real-world preventive maintenance scenarios. These results highlight the importance of both model selection and threshold tuning when working with rare but costly events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0eaf0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "perm = permutation_importance(\n",
    "    rf,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    n_repeats=10,\n",
    "    random_state=42,\n",
    "    scoring=\"average_precision\"\n",
    ")\n",
    "\n",
    "importances = pd.DataFrame({\n",
    "    \"feature\": X_test.columns,\n",
    "    \"importance_mean\": perm.importances_mean,\n",
    "    \"importance_std\": perm.importances_std\n",
    "}).sort_values(\"importance_mean\", ascending=False)\n",
    "\n",
    "importances.head(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ea5654",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n = 15\n",
    "top = importances.head(top_n).sort_values(\"importance_mean\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(top[\"feature\"], top[\"importance_mean\"])\n",
    "plt.xlabel(\"Permutation Importance (Î” average precision)\")\n",
    "plt.title(f\"Top {top_n} Features by Permutation Importance (Random Forest)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e8e3c0",
   "metadata": {},
   "source": [
    "Permutation importance highlights which signals most strongly influence failure predictions in the tuned random forest model. Rotational speed, torque, temperature differential, and tool wear dominate predictive performance, indicating that mechanical stress and accumulated wear are primary drivers of machine failure. Categorical product type contributes minimally once operational conditions are accounted for, suggesting that failure risk is largely governed by usage patterns rather than product classification."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
